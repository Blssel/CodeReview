# 前言
沿着CNN的发展历史，将几种经典的卷积神经网络做一个梳理，揣摩每一种网络设计之初的构>    想以及相对应采用的解决方案。然后结合CNN在目标检测上的应用做一个总结。
# 12年以后的发展脉络
  
# ResNet
ResNet[1]网络出现于2015年ImageNet图像分类大赛上，由微软Kaiming He等人提出，在原有网路的基础之上增加了若干跨层的短连接，在不增加计算消耗的情况下使得网络可以做的更深，并且性能有了大幅提升。对应论文发表于CVPR2015.
## Motivation
在ResNet出现之前，卷积神经网络的发展有这么一个趋势，那就是研究者倾向于将网络越做越深，因为从理论上来讲，更多层数的堆叠会使得网络具备更强的非线性拟合能力。但实验现象并不符合我们的预期，首先，更深的网络会引来梯度消失和梯度爆炸的问题[resnet1,9];其次，实验现象印证，随着层数的增加，网络的分类精度越发趋向于饱和，然后开始快速下降，如图1所示。

关于梯度消失和爆炸问题，研究者已经找到了许多相当好的解决方案，比如normalized initialization[resnet 23,9,37,13]和intermediate normalization layers[16]，使得SGD可以让网络很好的收敛。ResNet主要解决性能退化(degradation)问题，导致性能退化的主要原因是更深的网络使得训练变得非常困难，He等人做了一个有意思的实验，他们在已有的浅层网络基础上增加了若干恒等映射(identity mapping)来使得网络变得更深，所谓恒等映射，按笔者理解，应该是由num_channels个1*1*num_channels的卷积核构成的，其中每个卷积核都是只有某一位为1其它位置全为0的稀疏向量，比如(1,0,0,0,...,0)。从理论上说，该网络和浅层网络是等价的，因此性能至少不会比对应的浅层网络差，但事实证明其训练误差要比浅层的大，这就印证了一个规律：太深的网络会增加优化难度，而且这种难度并不是由过拟合造成的，因为其训练误差比浅层的还大。
  
## 解决方案
为解决过深网络存在的degradation问题，He等人设计了一个残差网络(Residual Network)。设计思想是：在网络中的某一层或隔层之间额外增加一个恒等映射，并且寄希望于让该层或几层去学习残差，而非直接学习直接的映射关系。
形式化的表达如下：假设要拟合一个映射$H(x)$，我们不直接学习，而是去拟合另一个映射$F(x)=H(x)-x$，然后再加上$x$，这样就达到了同样的效果，即$F(x)+x$等价于$H(x)$。这么做是基于这样一个假设，即学习残差比直接学习映射要容易。一个极端情况是，如果需要学习一个恒等映射，学习一个0残差比直接学习映射要简单许多。总之，构建的模块形式化表达为为下式：
$y=F(x,{W_i})+x$

实现在网络中则采用如下方式，即在层之间增加一个短连接(shortcut connections)，如图2所示。短连接的作用就是加了一个跨层的恒等映射，如此一来，短连接之间的映射就变成了一个残差映射。图2所示是一个跨2层的短连接，残差映射部分的数学表达为$$F=W_2 o(W_1 x)$$，最后$$F$$与$$x$$做一个元素级别的相加。值得注意的是，$$F$$和$$x$$的维度有可能不相同，在这种情况下，我们采用$$y=F(x,{W_i})+W_s x$$的方式，即给$$x$$做一个映射。如果是全连接网络，直接匹配输入层维度即可；但如果是卷积网络，则要么采用zero padding的方式强制保持维度一直，要么采用1*1，stride=2的卷积核，使得维度保持一致（注：此处的维度不一致专指通道数，因为采用3*3的卷积核加padding，feature map的长宽尺寸保持不变）

## 效果
实验证明，ResNet所采用的思想不仅使得优化变得容易，而且在不增加参数量的情况下提升了预测精度。更值得注意的是，ResNet支持将网络做的非常深，甚至可以达到1000层。

## Tensorflow实现ResNet代码

## TF-slim版代码

